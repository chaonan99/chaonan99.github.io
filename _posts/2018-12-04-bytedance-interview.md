---
layout: post
title: "Memo for the Interview of ByteDance"
comments: true
date: "2018-12-04"
description: "This post is a memo of what happened in my interview with ByteDance the other day"
keywords: "interview, memo, bytedance"
---

I have a video interview last Friday (Nov. 30). I was applying for a research position at [ByteDance AI Lab](https://bytedance.com/ai_lab/) in North America. The interviewer was, I guess, a research scientist there. The first topic, and also a large part of the interview, is talking about my current research.
It turns out that he is from speech processing community we both know little about other's field. But I think I've described my research as clear and interesting.

Apart from my research, I was also asked to write a convolution function with for loop. I haven't been playing with [CNNs](http://cs231n.github.io/convolutional-networks/) for quite a time and it seems that there are many variants of convolution operations. I think if one wanted to succeed in the interview on deep learning, especially computer vision, he or she should be more familiar with [some concepts in convolution layer](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d) like stride, padding, etc. than I was.

One last question was about how I view the relation between traditional logic based AI and current deep learning method. I guess that question was inspired by recent Twitter discussion between these two. My answer was that deep learning can handle what logic can do, given particularly designed architecture. But I think that's only some random answer. I was thinking about some [recent work](https://arxiv.org/pdf/1803.03067.pdf) by Chris Manning on machine reasoning, but I cannot remember the detail of their task at that time.